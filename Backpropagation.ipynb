{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dress-kenya",
   "metadata": {},
   "source": [
    "# Background \n",
    "\n",
    "- Türevin temelleri\n",
    "\n",
    " \\begin{equation}\n",
    "     \\frac {d(e{^u})}{dx}= e{^u} \\frac {du}{dx}\n",
    "  \\end{equation}\n",
    "\n",
    "  \\begin{equation}\n",
    "     \\frac {d(g+h)}{dx}= \\frac {dg}{dx} + \\frac {dh}{dx} \n",
    "  \\end{equation}\n",
    "\n",
    "  \\begin{equation}\n",
    "     \\frac {d(g{^n})}{dx}=  ng{^{n-1}}\\frac{dg}{dx} \n",
    "  \\end{equation}\n",
    "  \n",
    "- Geri yayılım öğrenme algoritmasını, toplam kare hatası üzerinde gradyan inişi olarak motive edebiliriz (hatanın karesini alırız çünkü işaretiyle değil, büyüklüğüyle ilgileniriz). Bir ağdaki toplam hata aşağıdaki denklemle verilir.\n",
    "\n",
    "    \\begin{equation}\n",
    "      E= \\frac {1}{2} (t_k-f_k)^2 \n",
    "    \\end{equation}\n",
    "\n",
    "- Bu genel hatayı azaltmak için ağın ağırlıklarını ayarlamak istiyoruz. Çıktı katmanında belirli bir ağırlıkla başlayacağız.Ancak hata doğrudan bir ağırlığın fonksiyonu değildir. Bunu aşağıdaki gibi genişletiyoruz. (Chain Rule)\n",
    "\n",
    "    \\begin{equation}\n",
    "      \\Delta W_{kj}= -\\epsilon \\frac {\\partial E}{\\partial f_k} \\frac {\\partial f_k}{\\partial net_k} \\frac {\\partial net_k}{\\partial w_{kj}}\n",
    "    \\end{equation}\n",
    "\n",
    "- Bu kısmi türevlerin her birini sırayla ele alalım.\n",
    "\n",
    "    - \\begin{equation}\n",
    "       \\frac {\\partial E}{\\partial f_k} = \\frac {\\partial ( \\frac {1}{2} (t_k-f_k)^2)}{\\partial f_k} = - (t_k-f_k)\n",
    "      \\end{equation}\n",
    "\n",
    "    - \\begin{equation}\n",
    "       \\frac {\\partial f_k}{\\partial net_k} = \\frac {\\partial (1+e^{-net_k})^{-1}}{\\partial net_k} = \\frac {e^{-net_k}}{(1+e^{-net_k})^2} \n",
    "      \\end{equation}\n",
    "        \n",
    "        Bu sonucu aktivasyon fonksiyonu açısından yeniden yazabilmek istiyoruz. \n",
    "   \n",
    "        \\begin{equation}\n",
    "          1 - \\frac {1}{1+e^{-net_k}} = \\frac {e^{-net_k}}{1+e^{-net_k}}\n",
    "        \\end{equation}\n",
    "        \n",
    "        olduğu için sonuç:\n",
    "        \n",
    "        \\begin{equation}\n",
    "          f_k(1-f_k)\n",
    "        \\end{equation}\n",
    "        \n",
    "  - \\begin{equation}\n",
    "      \\frac {\\partial net_k}{\\partial w_{kj}} = \\frac {\\partial w_{kj}x_j}{\\partial w_{kj}} = x_j\n",
    "    \\end{equation}\n",
    "        \n",
    "- Şimdi bu sonuçları orijinal denklemimize geri koyarsak,\n",
    "\n",
    "   \\begin{equation}\n",
    "      \\Delta W_{kj}= \\epsilon (t_k-f_k)f_k(1-f_k)x_j\n",
    "    \\end{equation}\n",
    "    \n",
    "- aktivasyon fonksiyonunun türevi ile hatanın çarpımını $\\gamma$ ile gösterirsek;\n",
    "\n",
    "    \\begin{equation}\n",
    "      \\gamma = (t_k-f_k)f_k(1-f_k)\n",
    "    \\end{equation}\n",
    "    \n",
    "- Son olarak bu şekilde bulmuş oluruz.\n",
    "\n",
    "     \\begin{equation}\n",
    "      \\Delta w_{kj} = \\epsilon*\\gamma_k*x_j\n",
    "     \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-student",
   "metadata": {},
   "source": [
    "# Hatayı Geriye Yayma\n",
    "### İlk ağırlıklar 0’a yakın rasgele küçük değerler atanır.\n",
    "\n",
    "### Aktifleşmenin hesaplanması\n",
    "  - Ağa ilk giriş değerleri verilir.\n",
    "  - Gizli katmandaki her bir nöron için aktifleşme aşağıdaki ifadelere göre hesaplanılır:\n",
    "      - Ağırlıkla girişin çarpılması ve biasın toplanması\n",
    "  \\begin{equation}\n",
    "     \\sum (wx+b)\n",
    "  \\end{equation}\n",
    "            \n",
    "      - Örnek sigmoid fonksiyon\n",
    "  \\begin{equation}\n",
    "     \\frac{1}{1+e^{-n}}\n",
    "  \\end{equation}\n",
    "  \n",
    "  - Çıktı katmanı da aynı gizli katman gibi hesaplanır. Gizli katmanın bir önceki adımında hesaplanmış olan çıkış değerleri bu ifadelerde çıkış nöronunun girişleri olacaktır.\n",
    "  \n",
    "### Hatanın hesaplanması\n",
    "\n",
    "  - Her bir çıkış nöronu için geriye yayılma hatasının hesaplanması. Burada $f_k$ gizli katmandaki k. nöronun hesaplanmış olan çıkış değeridir.\n",
    "  \n",
    "     \\begin{equation}\n",
    "      \\gamma_k = f_k*(1-f_k)*(t_k-f_k)\n",
    "     \\end{equation}\n",
    "     \n",
    "  - Bu adımda önce verilmiş bütün çıkış hedefleri ile elde edilmiş çıkış değerleri kıyaslanır. Eğer bu değerler farklı ise hesaplama devam eder. Farklı değilse hesaplama bitmiştir. Kayıp Fonksiyon iterasyonu durdurmada etkilidir. Kayıp Fonksiyon olarak **Mean Square Error** seçilmiş ise aşağıdaki denkeleme göre kayıp fonksiyon bulunur:\n",
    "  \n",
    "    \\begin{equation}\n",
    "      E= \\frac {1}{2} \\sum (t_k-f_k)^2 \n",
    "    \\end{equation}\n",
    "    \n",
    "  - Bütün giriş kombinasyonları için uygun çıkışların E değeri “0” veya bu değer önceden verilmiş bir hata değerinden eşit veya küçük olduğunda eğitim bitirilir. Gizli katmanlardan en son katmandaki nöronlar için hatanın hesaplanması. Burada $f_k$ bir önceki gizli katmandaki nöronların çıkış değerleridir. Eğer gizli katman bir tane ise, o zaman $f_k$ yerine $x_k$ yani uygun giriş değerleri kullanılır. \n",
    "    \n",
    "    \n",
    "### Yeni ağırlıkların hesaplanması\n",
    "\n",
    "   - Bütün katmanlar için ağırlık değerleri yeniden hesaplanılır. Optimizer olarak **Stochastic Gradient Descent** seçilmiş ise bunun için aşağıdaki ifadeler kullanılır. Burada $\\epsilon$, eğitim hızı (0 < $\\epsilon$ < 1, örneğin $\\epsilon$=0,3); $\\gamma i$, i. birimin hatasıdır.\n",
    "   \n",
    "      \\begin{equation}\n",
    "       \\Delta w_{ki} = \\epsilon*\\gamma_i*x_k\n",
    "      \\end{equation}\n",
    "      \n",
    "      \\begin{equation}\n",
    "       w_{ki}^{(t+1)} = w_{ki}^{(t)} - \\Delta w_{ki}\n",
    "      \\end{equation}\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-vienna",
   "metadata": {},
   "source": [
    "# Örnek 1 Epok\n",
    "\n",
    "![](imgs/ysa.png)\n",
    "\n",
    "- Burada Xb bias girişidir ve her zaman X_b=1 olmaktadır. \n",
    "- Ağırlıkların değerlerini aşağıdaki gibi kabul edelim:\n",
    "  - $W_{25}=-0.1  $                   \n",
    "  - $W_{24}=-0.2 $                 \n",
    "  - $W_{15}=0.1  $                  \n",
    "  - $W_{14}=0.1 $\n",
    "  - $W_{06}=0.3$\n",
    "  - $W_{05}=0.3 $                     \n",
    "  - $W_{04}=0.1$                    \n",
    "  - $W_{56}=0.3$                       \n",
    "  - $W_{46}=0.2$                    \n",
    "- Giriş değerlerini:\n",
    "  - $X_1=0.9$\n",
    "  - $X_2=0.1$   \n",
    "- Çıkış değeri \n",
    "  - $Y=0.9 $\n",
    "- Öğrenme oranı $\\epsilon = 0.25$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-daniel",
   "metadata": {},
   "source": [
    "###  Feed Forward\n",
    "\n",
    "- $net_4 = (1.0 × 0.1 ) + ( 0.1 × -0.2 ) + ( 0.9 × 0.1 ) = 0.170$\n",
    "  - $out_4 = \\frac {1}{( 1 + e^{( -0.170 )} )} = 0.542$\n",
    "\n",
    "- $net_5 = ( 1.0 × 0.1 ) + ( 0.1 × -0.1 ) + ( 0.9 × 0.3 ) = 0.360$\n",
    "  - $out_5 = \\frac {1} {( 1 + e^{ ( -0.360 )} )} = 0.589$\n",
    "\n",
    "- $net_6 = ( 1.0 × 0.2 ) + ( 0.542 × 0.2 ) + ( 0.589 × 0.3 ) = 0.485$\n",
    "  - $out_6 = \\frac {1}{( 1 + e^{(-0.485)})} = 0.619$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-dream",
   "metadata": {},
   "source": [
    "### Calculate Error\n",
    "\n",
    "- $\\gamma_6= 0.619 × (1.0 - 0.619) × (0.9 – 0.619)=0.066$\n",
    "    - $E = 0.5 × (0.9 – 0.619 )^{2} =0.5 × 0.078961 = 0.03948$ E ≠ 0 olduğu için hesaplama devam edecektir.\n",
    "- $\\gamma_5 = 0.589 × (1.0 - 0.589) × (0.066 × 0.3)=0.005$\n",
    "- $\\gamma_4 = 0.542 × (1.0 - 0.542) × (0.066 × 0.2)=0.003$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-benjamin",
   "metadata": {},
   "source": [
    "### Back propagation\n",
    "\n",
    "- $\\Delta w_56 = 0.25 × 0.066 × 0.589 = 0.01$\n",
    "    - $w_{56}^{(t+1)} = w_{56}^{(t)} - \\Delta w_{56} = 0.3 + 0.01 = 0.31$\n",
    "- $\\Delta w_{46} = 0.25 × 0.066 × 0.542 = 0.009$\n",
    "    - $w_{46}^{(t+1)} = 0.2 - 0.009 = 0.209$\n",
    "- $\\Delta w_{06} = 0.25 × 0.066 × 1.0 = 0.017$\n",
    "    - $w_{06}^{(t+1)} = 0.2 - 0.017 = 0.217$\n",
    "- $\\Delta w_{15} = 0.25 × 0.005 × 0.9 = 0.001125$\n",
    "    - $w_{15}^{(t+1)} = 0.1 - 0.001125= 0.1001125$\n",
    "- $\\Delta w_{14} = 0.25 × 0.003 × 0.9 = 0.000675$\n",
    "    - $w_{14}^{(t+1)} = 0.1 - 0.000675= 0.1000675$\n",
    "- $\\Delta w_{25} = 0.25 × 0.005 × 0.1 = 0.000125$\n",
    "    - $w_{25}^{(t+1)} = -0.1 - 0.000125= -0.999875$\n",
    "- $\\Delta w_{24} = 0.25 × 0.003 × 0.1 = 0.000075$\n",
    "    - $w_{24}^{(t+1)} = -0.1 - 0.000075= -0.999925$\n",
    "- $\\Delta w_{05} = 0.25 × 0.005 × 1 = 0.00125$\n",
    "    - $w_{05}^{(t+1)} = 0.3 - 0.000125= 0.300125$\n",
    "- $\\Delta w_{04} = 0.25 × 0.003 × 1 = 0.00125$\n",
    "    - $w_{04}^{(t+1)} = 0.1 - 0.00125= 0.10125$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-austin",
   "metadata": {},
   "source": [
    "# Örnek 2 XOR\n",
    "\n",
    "![](imgs/xor.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-reputation",
   "metadata": {},
   "source": [
    "### 1.Epok 1. İterasyon\n",
    "\n",
    "- *Çıkış katmanındaki 1 nolu nöronun aktifleşmesi:*\n",
    "- $net_1=(0,02 ×(-0.02) + 1 × 0.02 + 1 × 0.03 +  1 × (-0.01) = -0.0396$\n",
    "- $out_1= y_1 = \\frac {1}{( 1 + e^{-0.0396})} =  0.509$\n",
    "  - Çıkış hatası: $E=0.5(0-0.509)^{2}=0.1295$ Çıkış hatası “0” olmadığı için eğitim devam etmelidir.\n",
    "- *Çıkış nöronunun geriye yayılma hatasını hesaplayalım:*\n",
    "- $\\gamma_1 = 0.512 × (1- 0.512) × (0-0.512)= - 0.1219$.\n",
    "\n",
    "- *Gizli katmandaki 2. nöronun aktifleşmesi:*\n",
    "- $net_2=(1 × 0.01 + 1 × 0.02 + 1 × 0.01)=0.02$\n",
    "- $out_2 = y_2 =  \\frac {1}{(1+e^{-0.02})} = 0.512$\n",
    "- *Gizli katmandaki nöronunun geriye yayılma hatasını hesaplayalım:*\n",
    "- $\\gamma_2 = 0.512 × (1- 0.512) × ((-0.1219 × (-0.02))= 0.000609$\n",
    "\n",
    "- *Ağırlıkların yeni değerlerini hesaplayalım:*\n",
    "- $\\Delta W_{12} = \\epsilon × \\gamma_1 × y_2 = 0.3 × (–0.1219) × 0.512= -0.01872$\t\n",
    "  - $w_{12}^{+} = -0.02 - (-0.01872)= $\n",
    "- $\\Delta W_{13} = \\epsilon × \\gamma_1 × X_1 = 0.3 (–0.1219) 1 = -0.03657\t$\n",
    "  - $w_{13}^{+} = 0.02 - (–0.03657) = $\n",
    "- $\\Delta W_{23} = \\epsilon × \\gamma_2 × X_2 = 0.3 × 0.000609 × 1 = 0.0001827 $\t\n",
    "  - $w_{23}^{+} = 0.01 - (–0.0001827) = $\n",
    "- $\\Delta W_{14} = \\epsilon × \\gamma_1 × X_2 = 0.3 × (-0.1219) × 1 = -0.03657 $\t\n",
    "  - $w_{14}^{+} = 0.03 - (–0.03657) = $\n",
    "- $\\Delta W_{24} = \\epsilon × \\gamma_2 × X_2 = 0.0001827 $\t\n",
    "  - $w_{24}^{+} = 0.02 - (0.0001827) = $\n",
    "- $\\Delta W_{1b} = \\epsilon × \\gamma_2 × 1= 0.0001827 $\t\n",
    "  - $w_{1b}^{+} = -0.01 - (–0.0001827) = $\n",
    "- $\\Delta W_{2b} = \\epsilon × \\gamma_2 × 1 = 0.0001827 $\t\n",
    "  - $w_{2b}^{+} = -0.01 - 0.0001827 = $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-testimony",
   "metadata": {},
   "source": [
    "###  1.Epok 2. İterasyon\n",
    "\n",
    "- Bu hesaplamalardan sonra $X_1=1$ ve $X_2=0$ ve $y=1$ çıkışı için elde ettiğimiz en son ağırlık değerleri kullanılarak yeni hesaplamalar yapılır ve bu adım için yeni ağırlık değerleri bulunur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-berry",
   "metadata": {},
   "source": [
    "# Örnek 3\n",
    "\n",
    "![](imgs/ysa2.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-paradise",
   "metadata": {},
   "source": [
    "### Feed Forward\n",
    "\n",
    "  \\begin{equation} \n",
    "    net_{h1} = w_1 * i_1 + w_2 * i_2 + b_1 * 1 = 0.15 * 0.05 + 0.2 * 0.1 + 0.35 * 1 = 0.3775  \n",
    "  \\end{equation}\n",
    "  \n",
    "  \\begin{equation} \n",
    "    out_{h1} = \\frac{1}{1+e^{-net_{h1}}} = \\frac{1}{1+e^{-0.3775}} = 0.593269992\n",
    "  \\end{equation}\n",
    "    \n",
    "  \\begin{equation} \n",
    "    net_{h2} = w_3 * i_1 + w_4 * i_2 + b_1 * 1 = 0.25 * 0.05 + 0.3 * 0.1 + 0.35 * 1 = 0.3925  \n",
    "  \\end{equation}\n",
    "  \n",
    "  \\begin{equation} \n",
    "    out_{h2} = \\frac{1}{1+e^{-net_{h1}}} = \\frac{1}{1+e^{-0.3925}} =  0,596884378\n",
    "  \\end{equation}\n",
    "  \n",
    "  \\begin{equation}\n",
    "   net_{o1} = w_5 * out_{h1} + w_6 * out_{h2} + b_2 * 1 = 0.4 * 0.593269992 + 0.45 * 0.596884378 + 0.6 * 1 = 1.105905967\n",
    "  \\end{equation}\n",
    "  \n",
    "  \\begin{equation}\n",
    "    out_{o1} = \\frac{1}{1+e^{-net_{o1}}} = \\frac{1}{1+e^{-1.105905967}} = 0.75136507\n",
    "  \\end{equation}\n",
    "\n",
    "  \\begin{equation}\n",
    "   net_{o2} = w_7 * out_{h1} + w_8 * out_{h2} + b_2 * 1 = 0.5 * 0.593269992 + 0.55 * 0.596884378 + 0.6 * 1 = 1.770567983\n",
    "  \\end{equation}\n",
    "  \n",
    "  \\begin{equation}\n",
    "    out_{o2} = \\frac{1}{1+e^{-net_{o2}}} = \\frac{1}{1+e^{-1.770567983}} = 0.772928465\n",
    "  \\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-carrier",
   "metadata": {},
   "source": [
    "### Calculate Error\n",
    "\n",
    "  \\begin{equation}\n",
    "    E_{total} = \\sum \\frac{1}{2}(target- output)^{2}\n",
    "  \\end{equation}\n",
    "  \n",
    "  \\begin{equation}\n",
    "    E_{o1} = \\frac{1}{2}(target_{o1} - out_{o1})^{2} = \\frac{1}{2}(0.01 - 0.75136507)^{2} = 0.274811083\n",
    "  \\end{equation}\n",
    "  \n",
    "  \\begin{equation}\n",
    "    E_{o2} = \\frac{1}{2}(target_{o2} - out_{o2})^{2} = \\frac{1}{2}(0.99 - 0.772928465)^{2} = 0.023560026\n",
    "  \\end{equation}\n",
    "  \n",
    "  \\begin{equation}\n",
    "    E_{total} = E_{o1} + E_{o2} = 0,274811083 + 0,023560026 = 0,298371109\n",
    "  \\end{equation}\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-distributor",
   "metadata": {},
   "source": [
    "### Back Propogation\n",
    "\n",
    "- Geri yayılım ile hedefimiz, ağdaki ağırlıkların her birini, gerçek çıktının hedef çıktıya daha yakın olmasına neden olacak şekilde güncellemek, böylece her bir çıktı nöronu ve bir bütün olarak ağ için hatayı en aza indirmektir.\n",
    "\n",
    "- Ağırlık $w_5$'e odaklanalım. $w_5$'teki bir değişikliğin toplam hatayı ne kadar etkilediğini bilmek istiyoruz. Dolayısıyla $\\frac{\\partial E_{total}}{\\partial w_{5}}$.  Okunuşu:  $E_{total}$'in $w_5$'ye göre kısmi türevi veya $w_5$'ye göre gradyanı.\n",
    "\n",
    "![](imgs/ysa3.PNG)\n",
    "\n",
    "\\begin{equation}\n",
    "  E_{total} = \\frac{1}{2}(target_{o1} - out_{o1})^{2} + \\frac{1}{2}(target_{o2} - out_{o2})^{2}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial E_{total}}{\\partial out_{o1}} = 2 * \\frac{1}{2}(target_{o1} - out_{o1})^{2 - 1} * -1 + 0\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial E_{total}}{\\partial out_{o1}} = -(target_{o1} - out_{o1}) = -(0.01 - 0.75136507) = 0.74136507\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial out_{o1}}{\\partial net_{o1}} = out_{o1}(1 - out_{o1}) = 0.75136507(1 - 0.75136507) = 0.186815602\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial net_{o1}}{\\partial w_{5}} = 1 * out_{h1} * w_5^{(1 - 1)} + 0 + 0 = out_{h1} = 0.593269992\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial E_{total}}{\\partial w_{5}} = \\frac{\\partial E_{total}}{\\partial out_{o1}} * \\frac{\\partial out_{o1}}{\\partial net_{o1}} * \\frac{\\partial net_{o1}}{\\partial w_{5}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial E_{total}}{\\partial w_{5}} = 0.74136507 * 0.186815602 * 0.593269992 = 0.082167041\n",
    "\\end{equation}\n",
    "\n",
    "- Hatayı azaltmak için, daha sonra bu değeri mevcut ağırlıktan çıkarırız (isteğe bağlı olarak, 0,5 olarak ayarlayacağımız bir öğrenme oranı ile çarpılır):\n",
    "\n",
    "\\begin{equation}\n",
    "w_5^{+} = w_5 - \\epsilon * \\frac{\\partial E_{total}}{\\partial w_{5}} = 0.4 - 0.5 * 0.082167041 = 0.35891648\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "w_6^{+} = 0.408666186\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "w_7^{+} = 0.511301270\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "w_8^{+} = 0.561370121\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-quebec",
   "metadata": {},
   "source": [
    "![](imgs/ysa4.PNG)\n",
    "\n",
    "- $out_{h1}$'in hem $out_{o1}$ hem de $out_{o2}$'yı etkilediğini biliyoruz, bu nedenle $\\frac{\\partial E_{total}}{\\partial out_{h1}}$'ın her iki çıktı nöronu üzerindeki etkisini de dikkate alması gerekiyor. :\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial E_{total}}{\\partial out_{h1}} = \\frac{\\partial E_{o1}}{\\partial out_{h1}} + \\frac{\\partial E_{o2}}{\\partial out_{h1}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial E_{o1}}{\\partial out_{h1}} = \\frac{\\partial E_{o1}}{\\partial net_{o1}} * \\frac{\\partial net_{o1}}{\\partial out_{h1}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial E_{o1}}{\\partial net_{o1}} = \\frac{\\partial E_{o1}}{\\partial out_{o1}} * \\frac{\\partial out_{o1}}{\\partial net_{o1}} = 0.74136507 * 0.186815602 = 0.138498562\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial net_{o1}}{\\partial out_{h1}} = w_5 = 0.40\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial E_{o1}}{\\partial out_{h1}} = \\frac{\\partial E_{o1}}{\\partial net_{o1}} * \\frac{\\partial net_{o1}}{\\partial out_{h1}} = 0.138498562 * 0.40 = 0.055399425\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial E_{o2}}{\\partial out_{h1}} = -0.019049119\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial E_{total}}{\\partial out_{h1}} = \\frac{\\partial E_{o1}}{\\partial out_{h1}} + \\frac{\\partial E_{o2}}{\\partial out_{h1}} = 0.055399425 + -0.019049119 = 0.036350306\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial out_{h1}}{\\partial net_{h1}} = out_{h1}(1 - out_{h1}) = 0.59326999(1 - 0.59326999 ) = 0.241300709\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial net_{h1}}{\\partial w_1} = i_1 = 0.05\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial E_{total}}{\\partial w_{1}} = \\frac{\\partial E_{total}}{\\partial out_{h1}} * \\frac{\\partial out_{h1}}{\\partial net_{h1}} * \\frac{\\partial net_{h1}}{\\partial w_{1}}  = (\\sum\\limits_{o}{\\frac{\\partial E_{total}}{\\partial out_{o}} * \\frac{\\partial out_{o}}{\\partial net_{o}} * \\frac{\\partial net_{o}}{\\partial out_{h1}}}) * \\frac{\\partial out_{h1}}{\\partial net_{h1}} * \\frac{\\partial net_{h1}}{\\partial w_{1}}  = (\\sum\\limits_{o}{\\gamma_{o} * w_{ho}}) * out_{h1}(1 - out_{h1}) * i_{1}  = \\gamma_{h1}i_{1}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial E_{total}}{\\partial w_{1}} = 0.036350306 * 0.241300709 * 0.05 = 0.000438568\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "w_1^{+} = w_1 - \\epsilon * \\frac{\\partial E_{total}}{\\partial w_{1}} = 0.15 - 0.5 * 0.000438568 = 0.149780716\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "w_2^{+} = 0.19956143\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "w_3^{+} = 0.24975114\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "w_4^{+} = 0.29950229\n",
    "\\end{equation}\n",
    "\n",
    "- Tüm ağırlıklar güncellenmiş oldu. Orijinal olarak 0,05 ve 0,1 girişleri ilettiğimizde ağdaki hata 0,298371109 idi. \n",
    "\n",
    "- Bu ilk geri yayılım turundan sonra, toplam hata şimdi 0.291027924'e düştü. \n",
    "\n",
    "- Bu işlemi 10.000 kez tekrarladıktan sonra, örneğin hata 0,0000351085'e düşüyor. \n",
    "\n",
    "- Bu noktada, 0,05 ve 0,1 ilettiğimizde, iki çıkış nöronu 0.015912196 (0.01 hedefe karşı) ve 0.984065734 (0.99 hedefe karşı) üretir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-liberty",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
